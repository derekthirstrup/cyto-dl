# Advanced Performance Configuration (Phase 3)
#
# Additional optimizations beyond gpu_optimized.yaml:
# - Flash Attention for ViT models
# - Quantization settings
# - Advanced profiling
# - Multi-GPU DDP optimizations
#
# Usage:
#   python cyto_dl/train.py \
#     experiment=im2im/labelfree \
#     performance=advanced \
#     trainer=gpu_optimized

# Enable all Phase 1 optimizations
defaults:
  - gpu_optimized

# Flash Attention for ViT models
flash_attention:
  enabled: true
  use_flash: true  # Use Flash Attention if available
  fallback_to_sdpa: true  # Fall back to PyTorch SDPA if Flash Attention unavailable

# Quantization settings (for deployment)
quantization:
  enabled: false  # Enable for inference/export only
  mode: "dynamic"  # Options: dynamic, static, qat
  backend: "fbgemm"  # fbgemm (server) or qnnpack (mobile)

# Advanced profiling
profiling:
  enabled: false  # Enable for profiling runs only
  profile_memory: true
  profile_cuda: true
  with_stack: false  # Set true for detailed stack traces (slower)
  record_shapes: true
  with_flops: true

# Multi-GPU DDP optimizations
distributed:
  sync_bn: true  # Synchronized BatchNorm across GPUs
  gradient_as_bucket_view: true  # Memory-efficient gradients
  static_graph: false  # Set true if model graph never changes
  find_unused_parameters: false  # Set true only if needed (slower)
  bucket_cap_mb: 25  # Gradient communication bucket size

  # Gradient compression
  gradient_compression:
    enabled: false  # Enable for multi-node training
    type: "powersgd"  # Options: powersgd, fp16, none
    powersgd_rank: 2  # Lower = more compression, 2-4 recommended

# Automated performance tuning
auto_tune:
  enabled: false  # Enable to auto-tune on first run
  tune_batch_size: true
  tune_num_workers: true
  tune_precision: true
  save_config: "auto_tuned_config.yaml"

# Memory management
memory:
  gradient_checkpointing: false  # Enable if OOM (trades compute for memory)
  empty_cache_every_n_steps: 0  # Set >0 if memory fragmentation issues
  max_split_size_mb: null  # Set to reduce memory fragmentation

# Expected performance improvements:
# - Flash Attention: +2-4x faster attention (ViT models)
# - Quantization (INT8): +2-4x faster inference
# - DDP optimizations: +10-20% faster multi-GPU
# - Gradient compression: +20-40% faster multi-node
# - Gradient checkpointing: -40-60% memory usage
#
# Combined with Phase 1: 5-10x total speedup possible
