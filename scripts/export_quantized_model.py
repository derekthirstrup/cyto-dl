"""Export trained CytoDL models with quantization for faster inference.

This script applies PyTorch native quantization (INT8) to models, providing
2-4x faster inference on both CPU and GPU.

Requirements:
    No extra dependencies - uses PyTorch built-in quantization

Usage:
    # Dynamic quantization (easiest, CPU-optimized)
    python scripts/export_quantized_model.py \
        --config configs/experiment/im2im/segmentation.yaml \
        --ckpt path/to/checkpoint.ckpt \
        --output model_quantized.pt \
        --mode dynamic

    # Static quantization (best accuracy, requires calibration)
    python scripts/export_quantized_model.py \
        --config configs/experiment/im2im/labelfree.yaml \
        --ckpt path/to/checkpoint.ckpt \
        --output model_quantized.pt \
        --mode static \
        --calibration-data path/to/calibration/images \
        --num-calibration-samples 100

    # QAT (Quantization-Aware Training, best quality)
    python scripts/export_quantized_model.py \
        --config configs/experiment/im2im/mae.yaml \
        --ckpt path/to/checkpoint.ckpt \
        --output model_qat.pt \
        --mode qat \
        --qat-epochs 5

    # Benchmark
    python scripts/export_quantized_model.py \
        --config configs/experiment/im2im/labelfree.yaml \
        --ckpt path/to/checkpoint.ckpt \
        --output model_quantized.pt \
        --mode dynamic \
        --benchmark
"""

import argparse
import sys
from pathlib import Path

import torch
from omegaconf import OmegaConf

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))


def parse_args():
    parser = argparse.ArgumentParser(description="Export quantized CytoDL model")

    # Model configuration
    parser.add_argument(
        "--config",
        type=str,
        required=True,
        help="Path to experiment config file",
    )
    parser.add_argument(
        "--ckpt",
        type=str,
        required=True,
        help="Path to model checkpoint",
    )
    parser.add_argument(
        "--output",
        type=str,
        required=True,
        help="Output path for quantized model (.pt or .ts file)",
    )

    # Quantization configuration
    parser.add_argument(
        "--mode",
        type=str,
        default="dynamic",
        choices=["dynamic", "static", "qat"],
        help="Quantization mode: dynamic, static, or qat",
    )
    parser.add_argument(
        "--backend",
        type=str,
        default="fbgemm",
        choices=["fbgemm", "qnnpack"],
        help="Quantization backend (fbgemm for server, qnnpack for mobile)",
    )

    # Static quantization configuration
    parser.add_argument(
        "--calibration-data",
        type=str,
        default=None,
        help="Path to directory with calibration images (for static/qat)",
    )
    parser.add_argument(
        "--num-calibration-samples",
        type=int,
        default=100,
        help="Number of calibration samples to use (default: 100)",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=4,
        help="Batch size for calibration (default: 4)",
    )

    # QAT configuration
    parser.add_argument(
        "--qat-epochs",
        type=int,
        default=5,
        help="Number of QAT fine-tuning epochs (default: 5)",
    )
    parser.add_argument(
        "--qat-lr",
        type=float,
        default=1e-5,
        help="Learning rate for QAT (default: 1e-5)",
    )

    # Export options
    parser.add_argument(
        "--torchscript",
        action="store_true",
        help="Export as TorchScript (.ts file)",
    )
    parser.add_argument(
        "--onnx",
        action="store_true",
        help="Also export to ONNX format",
    )

    # Benchmarking
    parser.add_argument(
        "--benchmark",
        action="store_true",
        help="Benchmark original vs quantized model",
    )
    parser.add_argument(
        "--benchmark-iterations",
        type=int,
        default=100,
        help="Number of benchmark iterations",
    )

    parser.add_argument(
        "--device",
        type=str,
        default="cuda" if torch.cuda.is_available() else "cpu",
    )

    return parser.parse_args()


def load_model_from_checkpoint(config_path: str, ckpt_path: str):
    """Load model from config and checkpoint."""
    import hydra
    from lightning import LightningModule

    # Load config
    cfg = OmegaConf.load(config_path)

    # Instantiate model
    print(f"Loading model from config: {config_path}")
    with hydra.initialize(config_path="../configs", version_base="1.3"):
        model: LightningModule = hydra.utils.instantiate(cfg.model, _recursive_=False)

    # Load checkpoint
    print(f"Loading checkpoint: {ckpt_path}")
    checkpoint = torch.load(ckpt_path, map_location="cpu")
    model.load_state_dict(checkpoint["state_dict"])
    model.eval()

    return model


def create_calibration_loader(
    data_path: str,
    num_samples: int,
    batch_size: int,
    input_shape: tuple,
):
    """Create calibration data loader."""
    from PIL import Image
    import numpy as np
    from torchvision import transforms
    from torch.utils.data import DataLoader, TensorDataset

    data_path = Path(data_path)
    calibration_tensors = []

    # Find image files
    image_files = list(data_path.glob("*.png")) + list(data_path.glob("*.jpg"))
    image_files = image_files[:num_samples]

    if len(image_files) == 0:
        raise ValueError(f"No images found in {data_path}")

    print(f"Loading {len(image_files)} calibration samples from {data_path}")

    # Create transform
    transform = transforms.Compose([
        transforms.Resize(input_shape[-2:]),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5], std=[0.5]),
    ])

    for img_file in image_files:
        img = Image.open(img_file).convert('L')
        img_tensor = transform(img).unsqueeze(0)

        # Adjust shape for 3D if needed
        while img_tensor.ndim < len(input_shape):
            img_tensor = img_tensor.unsqueeze(2)

        calibration_tensors.append(img_tensor)

    # Stack into dataset
    calibration_data = torch.cat(calibration_tensors, dim=0)
    dataset = TensorDataset(calibration_data)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

    return loader


def main():
    args = parse_args()

    print(f"Device: {args.device}")

    # Load model
    model = load_model_from_checkpoint(args.config, args.ckpt)
    model = model.to(args.device)

    # Import quantization utilities
    from cyto_dl.utils.quantization import (
        quantize_model_dynamic,
        quantize_model_static,
        QuantizationAwareTraining,
        benchmark_quantized_model,
    )

    print(f"\nQuantizing model with mode: {args.mode}")
    print(f"Backend: {args.backend}")

    quantized_model = None

    if args.mode == "dynamic":
        # Dynamic quantization (easiest)
        print("\nðŸ”§ Applying dynamic quantization...")
        quantized_model = quantize_model_dynamic(
            model,
            dtype=torch.qint8,
            output_path=args.output if not args.torchscript else None,
        )

    elif args.mode == "static":
        # Static quantization (requires calibration)
        if args.calibration_data is None:
            print("Error: Static quantization requires --calibration-data")
            sys.exit(1)

        print("\nðŸ”§ Preparing static quantization...")

        # Get input shape from model
        # For simplicity, assume standard shapes
        input_shape = (1, 1, 256, 256)  # Default

        # Create calibration loader
        calibration_loader = create_calibration_loader(
            args.calibration_data,
            args.num_calibration_samples,
            args.batch_size,
            input_shape,
        )

        print("ðŸ”§ Applying static quantization...")
        quantized_model = quantize_model_static(
            model,
            calibration_loader=calibration_loader,
            backend=args.backend,
            output_path=args.output if not args.torchscript else None,
        )

    elif args.mode == "qat":
        # Quantization-Aware Training
        if args.calibration_data is None:
            print("Error: QAT requires --calibration-data for training")
            sys.exit(1)

        print("\nðŸ”§ Preparing Quantization-Aware Training...")

        input_shape = (1, 1, 256, 256)
        train_loader = create_calibration_loader(
            args.calibration_data,
            args.num_calibration_samples,
            args.batch_size,
            input_shape,
        )

        print(f"ðŸ”§ Running QAT for {args.qat_epochs} epochs...")
        qat_trainer = QuantizationAwareTraining(
            model,
            backend=args.backend,
        )

        # Prepare for QAT
        qat_model = qat_trainer.prepare()

        # Simple training loop
        optimizer = torch.optim.Adam(qat_model.parameters(), lr=args.qat_lr)
        criterion = torch.nn.MSELoss()

        qat_model.train()
        for epoch in range(args.qat_epochs):
            total_loss = 0
            for batch_idx, (batch,) in enumerate(train_loader):
                batch = batch.to(args.device)

                optimizer.zero_grad()

                # Forward pass (self-supervised for simplicity)
                output = qat_model(batch)
                loss = criterion(output, batch)  # Reconstruction loss

                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            avg_loss = total_loss / len(train_loader)
            print(f"  Epoch {epoch + 1}/{args.qat_epochs}: Loss = {avg_loss:.4f}")

        # Convert to quantized
        print("ðŸ”§ Converting QAT model to quantized...")
        quantized_model = qat_trainer.convert()

        # Save
        if args.output and not args.torchscript:
            torch.save(quantized_model.state_dict(), args.output)
            print(f"âœ“ Saved QAT model to {args.output}")

    if quantized_model is None:
        print("Error: Quantization failed")
        sys.exit(1)

    print(f"\nâœ“ Successfully quantized model with {args.mode} quantization")

    # Export TorchScript if requested
    if args.torchscript:
        print("\nðŸ”§ Exporting to TorchScript...")
        try:
            # Need a sample input for tracing
            if args.mode == "static" or args.mode == "qat":
                # Use calibration data shape
                sample_input = torch.randn(1, 1, 256, 256)
            else:
                sample_input = torch.randn(1, 1, 256, 256)

            traced_model = torch.jit.trace(quantized_model.cpu(), sample_input.cpu())
            traced_model.save(args.output)
            print(f"âœ“ Saved TorchScript model to {args.output}")
        except Exception as e:
            print(f"Warning: TorchScript export failed: {e}")
            print("Falling back to state_dict save")
            torch.save(quantized_model.state_dict(), args.output)

    # Export ONNX if requested
    if args.onnx:
        onnx_path = Path(args.output).with_suffix(".onnx")
        print(f"\nðŸ”§ Exporting to ONNX: {onnx_path}")
        try:
            sample_input = torch.randn(1, 1, 256, 256)
            torch.onnx.export(
                quantized_model.cpu(),
                sample_input.cpu(),
                str(onnx_path),
                input_names=["input"],
                output_names=["output"],
                dynamic_axes={"input": {0: "batch_size"}, "output": {0: "batch_size"}},
            )
            print(f"âœ“ Saved ONNX model to {onnx_path}")
        except Exception as e:
            print(f"Warning: ONNX export failed: {e}")

    # Benchmark if requested
    if args.benchmark:
        print(f"\nðŸ” Running benchmark comparison...")
        results = benchmark_quantized_model(
            original_model=model,
            quantized_model=quantized_model,
            input_shape=(1, 1, 256, 256),
            num_iterations=args.benchmark_iterations,
            device=args.device,
        )

        # Save benchmark results
        import json
        results_file = Path(args.output).with_suffix(".benchmark.json")
        with open(results_file, "w") as f:
            json.dump(results, f, indent=2)
        print(f"âœ“ Benchmark results saved to {results_file}")

    print("\n" + "=" * 60)
    print("NEXT STEPS")
    print("=" * 60)
    print(f"1. Test the quantized model:")
    print(f"   >>> import torch")
    if args.torchscript:
        print(f"   >>> model = torch.jit.load('{args.output}')")
    else:
        print(f"   >>> model = MyModel()")
        print(f"   >>> model.load_state_dict(torch.load('{args.output}'))")
    print(f"   >>> output = model(torch.randn(1, 1, 256, 256))")
    print(f"")
    print(f"2. Compare accuracy:")
    print(f"   - Run inference on validation set")
    print(f"   - Compare metrics with original model")
    print(f"   - Expected: <1% accuracy loss for static/qat")
    print(f"")
    print(f"3. Deploy:")
    print(f"   - Use quantized model for production inference")
    print(f"   - Expect 2-4x speedup on CPU")
    print(f"   - Or combine with TensorRT for GPU (up to 8x total)")
    print("=" * 60)


if __name__ == "__main__":
    main()
